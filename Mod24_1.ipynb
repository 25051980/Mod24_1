{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1c4fb9-0684-4f3b-8d5e-91536c385254",
   "metadata": {},
   "source": [
    "# Modulo 24 Exercício 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adffc54-2fdd-4ba4-8edd-7bcf6e7fbaca",
   "metadata": {},
   "source": [
    "###  Diferenças entre Random Forest e AdaBoost\n",
    "\n",
    "1. **Estratégia de Conjunto**: O **Random Forest** combina diversas árvores de decisão independentes (bagging), enquanto o **AdaBoost** cria uma sequência de modelos fracos, ajustando cada modelo para corrigir os erros do anterior (boosting).\n",
    "2. **Peso dos Modelos**: No Random Forest, todas as árvores têm o mesmo peso na decisão final, enquanto no AdaBoost, cada modelo recebe um peso baseado em sua precisão, com modelos posteriores focando mais nos erros dos anteriores.\n",
    "3. **Treinamento dos Modelos**: Random Forest constrói todas as árvores de forma independente e simultânea, enquanto o AdaBoost treina os modelos de maneira sequencial, onde cada modelo depende do desempenho do anterior.\n",
    "4. **Robustez a Ruído**: O Random Forest tende a ser mais robusto ao ruído devido à diversidade das árvores independentes, enquanto o AdaBoost pode ser mais sensível a outliers, pois ajusta modelos para corrigir erros em instâncias mal classificadas.\n",
    "5. **Complexidade e Interpretação**: O Random Forest tende a ser mais fácil de interpretar, pois não ajusta o peso das instâncias, enquanto o AdaBoost é mais complexo devido à importância variável dada às instâncias e modelos ao longo das iterações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f87ebd-6c55-4fec-a761-413529b25608",
   "metadata": {},
   "source": [
    "## Exemplo do AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed066133-007a-46a6-a067-e96f4cda4a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média das pontuações da validação cruzada: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Carregando o conjunto de dados iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Inicializando o classificador AdaBoost com 100 estimadores e o algoritmo SAMME\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\")\n",
    "\n",
    "# Realizando validação cruzada com 5 divisões e calculando a média das pontuações\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(\"Média das pontuações da validação cruzada:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb12c5-8ac2-4a2f-b0ca-10e509fe746d",
   "metadata": {},
   "source": [
    "### Hiperparâmetros Principais do AdaBoost\n",
    "\n",
    "- n_estimators\n",
    "- learning_rate\n",
    "- estimator\n",
    "- max_depth (quando o estimador base é uma árvore de decisão)\n",
    "- min_samples_split (quando o estimador base é uma árvore de decisão)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf83a04-26d6-4a41-bbea-e354023e5a26",
   "metadata": {},
   "source": [
    "### Explicação do Código para Otimização com GridSearchCV\n",
    "\n",
    "Este código utiliza o **GridSearchCV** para encontrar os melhores hiperparâmetros do classificador **AdaBoost** no conjunto de dados Iris. Abaixo estão os passos principais:\n",
    "\n",
    "1. **Definição do Classificador**: Inicializamos o AdaBoost sem especificar hiperparâmetros, pois estes serão ajustados pelo GridSearch.\n",
    "2. **Grade de Hiperparâmetros** (`param_grid`): Configuramos uma lista de valores para cada hiperparâmetro do AdaBoost:\n",
    "   - `n_estimators`: Número de classificadores fracos (valores testados: 50, 100, 150).\n",
    "   - `learning_rate`: Taxa de aprendizado (valores testados: 0.1, 0.5, 1.0).\n",
    "   - `base_estimator`: Tipo de estimador base, aqui utilizamos árvores de decisão com profundidades 1 e 2.\n",
    "   - `algorithm`: Algoritmo de boosting, com opções \"SAMME\" e \"SAMME.R\".\n",
    "3. **Configuração do GridSearchCV**: Com o parâmetro `cv=5`, o GridSearch realiza uma validação cruzada de 5 vezes para cada combinação de hiperparâmetros, usando a acurácia como métrica de avaliação.\n",
    "4. **Ajuste do Modelo**: O `fit` ajusta o modelo e identifica a combinação de hiperparâmetros que maximiza a acurácia.\n",
    "5. **Resultados**: Exibimos os melhores hiperparâmetros (`best_params_`) e a melhor pontuação de acurácia (`best_score_`), representando a performance otimizada do modelo.\n",
    "\n",
    "Este processo ajuda a encontrar os parâmetros ideais para maximizar o desempenho do modelo no conjunto de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9b7b12-a70f-42c0-a5f0-2d0c22537e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad5d660-a51c-415f-9a31-0caa3f4cfa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros: {'algorithm': 'SAMME', 'base_estimator': DecisionTreeClassifier(max_depth=2), 'learning_rate': 1.0, 'n_estimators': 100}\n",
      "Melhor pontuação: 0.9600000000000002\n"
     ]
    }
   ],
   "source": [
    "# Carregando o conjunto de dados iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Definindo o classificador AdaBoost\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "# Definindo a grade de hiperparâmetros para testar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # número de estimadores\n",
    "    'learning_rate': [0.1, 0.5, 1.0],  # taxa de aprendizado\n",
    "    'base_estimator': [DecisionTreeClassifier(max_depth=1), \n",
    "                       DecisionTreeClassifier(max_depth=2)],  # estimador base com diferentes profundidades\n",
    "    'algorithm': ['SAMME', 'SAMME.R']  # escolha do algoritmo\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Ajustando o modelo para encontrar os melhores hiperparâmetros\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Exibindo os melhores hiperparâmetros encontrados\n",
    "print(\"Melhores hiperparâmetros:\", grid_search.best_params_)\n",
    "print(\"Melhor pontuação:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bedd6-483f-4ba3-b301-86aa3c58c734",
   "metadata": {},
   "source": [
    "### Resultados da Otimização com GridSearchCV\n",
    "\n",
    "Após realizar a busca de hiperparâmetros com o **GridSearchCV**, os melhores hiperparâmetros encontrados para o classificador AdaBoost foram:\n",
    "- `algorithm`: **SAMME**\n",
    "- `base_estimator`: **DecisionTreeClassifier** com `max_depth=2`\n",
    "- `learning_rate`: **1.0**\n",
    "- `n_estimators`: **100**\n",
    "\n",
    "Esses parâmetros resultaram na melhor pontuação de acurácia média de **0.96** (ou 96%) durante a validação cruzada de 5 vezes. Isso indica que, com essa configuração, o modelo atingiu um bom desempenho no conjunto de dados Iris, maximizando a capacidade de classificação correta entre as classes do dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
